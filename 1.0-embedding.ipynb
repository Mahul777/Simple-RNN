{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 6086 1649 3854 2592]\n",
      " [   0    0    0    0 6086 1649 3854 7355]\n",
      " [   0    0    0    0 6086 3942 3854 9267]\n",
      " [   0    0    0 3828 1193  872 5319 2893]\n",
      " [   0    0    0 3828 1193  872 5319 6764]\n",
      " [   0    0    0  391 6086 7592 3854  464]\n",
      " [   0    0    0    0 1672 6762  789 5319]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001E01D405D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "[[[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [ 2.57819034e-02  2.84010656e-02 -3.00602671e-02  3.12190130e-03\n",
      "    2.30179168e-02  1.68727748e-02  2.47445814e-02  7.81860203e-03\n",
      "    9.73502547e-03  7.07067177e-03]\n",
      "  [-3.84193063e-02 -8.83370638e-03  1.62816979e-02 -3.47054489e-02\n",
      "   -2.04754360e-02 -2.43184455e-02 -1.75613873e-02  4.30643559e-04\n",
      "   -4.03550044e-02 -2.84786709e-02]\n",
      "  [-8.42020661e-03  1.65116042e-04  2.92887576e-02  7.75305182e-03\n",
      "    2.85871141e-02 -3.28085646e-02 -1.49924271e-02 -7.91152567e-03\n",
      "   -1.81864500e-02  3.21807154e-02]\n",
      "  [ 5.93557209e-03  1.06701255e-02 -3.87862809e-02  3.20142247e-02\n",
      "    1.04553103e-02 -4.44733985e-02  1.71518587e-02  2.71410979e-02\n",
      "   -4.77702618e-02 -4.28810716e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [ 2.57819034e-02  2.84010656e-02 -3.00602671e-02  3.12190130e-03\n",
      "    2.30179168e-02  1.68727748e-02  2.47445814e-02  7.81860203e-03\n",
      "    9.73502547e-03  7.07067177e-03]\n",
      "  [-3.84193063e-02 -8.83370638e-03  1.62816979e-02 -3.47054489e-02\n",
      "   -2.04754360e-02 -2.43184455e-02 -1.75613873e-02  4.30643559e-04\n",
      "   -4.03550044e-02 -2.84786709e-02]\n",
      "  [-8.42020661e-03  1.65116042e-04  2.92887576e-02  7.75305182e-03\n",
      "    2.85871141e-02 -3.28085646e-02 -1.49924271e-02 -7.91152567e-03\n",
      "   -1.81864500e-02  3.21807154e-02]\n",
      "  [ 4.57419492e-02  5.90072945e-03 -4.25643921e-02  3.53678316e-03\n",
      "    3.09115313e-02  2.77086347e-03 -1.24119297e-02 -2.13778857e-02\n",
      "    3.14191245e-02  1.52250566e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [ 2.57819034e-02  2.84010656e-02 -3.00602671e-02  3.12190130e-03\n",
      "    2.30179168e-02  1.68727748e-02  2.47445814e-02  7.81860203e-03\n",
      "    9.73502547e-03  7.07067177e-03]\n",
      "  [ 5.68496063e-03 -1.25393756e-02 -1.22499578e-02 -3.22490111e-02\n",
      "   -3.80340703e-02 -4.00984287e-03  4.96898554e-02  1.58017017e-02\n",
      "    2.76924260e-02 -1.92048792e-02]\n",
      "  [-8.42020661e-03  1.65116042e-04  2.92887576e-02  7.75305182e-03\n",
      "    2.85871141e-02 -3.28085646e-02 -1.49924271e-02 -7.91152567e-03\n",
      "   -1.81864500e-02  3.21807154e-02]\n",
      "  [-1.00008249e-02 -3.58087942e-03 -4.22408953e-02  3.44148986e-02\n",
      "    2.78228186e-02  5.77304512e-03 -1.76679604e-02 -1.80576667e-02\n",
      "   -4.57456373e-02 -4.94044200e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-1.09773502e-02 -4.18368205e-02  6.70177862e-03 -3.89540419e-02\n",
      "   -1.91171169e-02  4.63604927e-03  1.37690641e-02 -3.51084247e-02\n",
      "   -6.07620925e-04  3.42398770e-02]\n",
      "  [-3.77910957e-02  2.38163583e-02 -5.64882904e-03 -2.31626518e-02\n",
      "    2.17262246e-02  1.77042224e-02 -4.18933146e-02 -4.15177457e-02\n",
      "   -4.18173447e-02  3.36524881e-02]\n",
      "  [-1.15506724e-03  7.45617226e-03  4.21399213e-02 -4.42032926e-02\n",
      "   -8.95126909e-03  1.87438019e-02  2.22986005e-02  1.86494850e-02\n",
      "   -1.39845833e-02 -4.22341004e-02]\n",
      "  [-2.25221161e-02  2.25223042e-02  3.99779715e-02  5.71037456e-03\n",
      "    3.06345113e-02 -4.03459780e-02  2.62580626e-02 -4.30185199e-02\n",
      "   -4.41364311e-02  1.01390705e-02]\n",
      "  [-6.67632744e-03  3.21118720e-02 -2.01201569e-02  2.97880806e-02\n",
      "    2.04857923e-02  2.62092799e-04  1.03928931e-02  1.11756809e-02\n",
      "   -4.40913439e-02  4.54499610e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-1.09773502e-02 -4.18368205e-02  6.70177862e-03 -3.89540419e-02\n",
      "   -1.91171169e-02  4.63604927e-03  1.37690641e-02 -3.51084247e-02\n",
      "   -6.07620925e-04  3.42398770e-02]\n",
      "  [-3.77910957e-02  2.38163583e-02 -5.64882904e-03 -2.31626518e-02\n",
      "    2.17262246e-02  1.77042224e-02 -4.18933146e-02 -4.15177457e-02\n",
      "   -4.18173447e-02  3.36524881e-02]\n",
      "  [-1.15506724e-03  7.45617226e-03  4.21399213e-02 -4.42032926e-02\n",
      "   -8.95126909e-03  1.87438019e-02  2.22986005e-02  1.86494850e-02\n",
      "   -1.39845833e-02 -4.22341004e-02]\n",
      "  [-2.25221161e-02  2.25223042e-02  3.99779715e-02  5.71037456e-03\n",
      "    3.06345113e-02 -4.03459780e-02  2.62580626e-02 -4.30185199e-02\n",
      "   -4.41364311e-02  1.01390705e-02]\n",
      "  [-2.56937146e-02 -2.84427647e-02  3.05641927e-02 -2.25720759e-02\n",
      "    4.31428216e-02 -7.60761648e-03  1.16251335e-02  9.13452357e-04\n",
      "   -3.24382633e-03 -4.67516296e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [ 2.55730003e-03 -3.88851985e-02 -4.32846546e-02 -1.59139149e-02\n",
      "   -2.32142210e-02  2.60689147e-02  8.88149813e-03 -4.89567891e-02\n",
      "    1.16572492e-02  4.68645580e-02]\n",
      "  [ 2.57819034e-02  2.84010656e-02 -3.00602671e-02  3.12190130e-03\n",
      "    2.30179168e-02  1.68727748e-02  2.47445814e-02  7.81860203e-03\n",
      "    9.73502547e-03  7.07067177e-03]\n",
      "  [ 4.51591052e-02 -4.23606299e-02 -3.24082375e-02  2.45627277e-02\n",
      "   -3.55475768e-02 -2.03141216e-02  1.08781084e-02 -7.99167156e-03\n",
      "   -1.57973915e-03 -1.95351727e-02]\n",
      "  [-8.42020661e-03  1.65116042e-04  2.92887576e-02  7.75305182e-03\n",
      "    2.85871141e-02 -3.28085646e-02 -1.49924271e-02 -7.91152567e-03\n",
      "   -1.81864500e-02  3.21807154e-02]\n",
      "  [-2.04875953e-02  2.03506835e-02  2.02003866e-03  3.49645875e-02\n",
      "    4.53466512e-02 -2.82798652e-02 -8.02055001e-05 -7.12275505e-05\n",
      "   -3.78052965e-02  3.98858227e-02]]\n",
      "\n",
      " [[-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [-4.23709303e-03 -3.46774943e-02 -2.14374065e-02  3.88695113e-02\n",
      "    2.40114816e-02  1.75497644e-02  2.93568633e-02 -3.54238264e-02\n",
      "    7.68346712e-03 -3.11110504e-02]\n",
      "  [ 2.67025940e-02  3.07076909e-02 -4.75076437e-02  1.59784891e-02\n",
      "   -4.60231304e-03 -3.39876562e-02 -4.35254462e-02 -4.82290648e-02\n",
      "    4.02732156e-02  2.48824693e-02]\n",
      "  [-3.61581072e-02 -2.66533624e-02 -1.12390295e-02 -3.26871648e-02\n",
      "    2.92231701e-02  4.47819568e-02 -5.99632412e-03  4.36299331e-02\n",
      "    4.01424207e-02  3.92863639e-02]\n",
      "  [-3.46965082e-02 -4.42340858e-02 -1.69944018e-04  3.22343744e-02\n",
      "   -7.66049698e-03  2.14868300e-02  3.50823253e-03  4.46146391e-02\n",
      "    4.09974568e-02  2.09679343e-02]\n",
      "  [-2.25221161e-02  2.25223042e-02  3.99779715e-02  5.71037456e-03\n",
      "    3.06345113e-02 -4.03459780e-02  2.62580626e-02 -4.30185199e-02\n",
      "   -4.41364311e-02  1.01390705e-02]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 6086, 1649, 3854, 2592], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🔹 Step 2: ✅ Setup Environment & Libraries\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "# 🔹 Step 3: 📝 Prepare Sentences (Input Data)\n",
    "### sentences\n",
    "sent=[ 'the glass of milk',\n",
    "'the glass of juice',\n",
    "'the cup of tea',\n",
    "'I am a good boy',\n",
    "'I am a good developer',\n",
    "'understand the meaning of words',\n",
    "'your videos are good',]\n",
    "\n",
    "# •\t→ These are the example sentences you'll be working with.\n",
    "# •\t→ Each sentence will be converted into vector format.\n",
    "\n",
    "sent\n",
    "# ['the glass of milk',\n",
    "#  'the glass of juice',\n",
    "#  'the cup of tea',\n",
    "#  'I am a good boy',\n",
    "#  'I am a good developer',\n",
    "#  'understand the meaning of words',\n",
    "#  'your videos are good']\n",
    "\n",
    "# 🔹 Step 4: 📏 Vocabulary Size\n",
    "vocab_size = 10000\n",
    "# •\t→ This means you assume a vocabulary of 10,000 unique words.\n",
    "# •\t→ Useful for indexing words during encoding.\n",
    "\n",
    "# 🔹 Step 5: 🔢 One Hot Encoding (Index-Based)\n",
    "one_hot_representation = [one_hot(words,vocab_size) for words in sent] \n",
    "\n",
    "# Explain the above code\n",
    "# •\t→ This is a list comprehension that applies the one_hot function to each word in the sentence.\n",
    "# •\t→ The one_hot function converts each word into a one-hot representation.\n",
    "# •\t→ The one_hot function takes two arguments: the word and the vocabulary size.\n",
    "# •\t→ The one_hot function returns a one-hot representation of the word.\n",
    "\n",
    "one_hot_representation\n",
    "# [[6086, 1649, 3854, 2592],\n",
    "#  [6086, 1649, 3854, 7355],\n",
    "#  [6086, 3942, 3854, 9267],\n",
    "#  [3828, 1193, 872, 5319, 2893],\n",
    "#  [3828, 1193, 872, 5319, 6764],\n",
    "#  [391, 6086, 7592, 3854, 464],\n",
    "#  [1672, 6762, 789, 5319]]\n",
    "\n",
    "\n",
    "## word Embedding Representation\n",
    "# Step 6: 📏 Embedding Layer\n",
    "from tensorflow.keras.layers import Embedding # 📏 Embedding Layer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences # 📏 Pad Sequences Layer is used to pad the sequences to the same length.\n",
    "from tensorflow.keras.models import Sequential # 📏 Sequential Model is used to create a sequential model. \n",
    "import numpy as np\n",
    "\n",
    "# 🔹 Step 7: ⏩ Pad Sequences (Equal Length)\n",
    "sent_length = 8\n",
    "embedded_docs = pad_sequences(one_hot_representation, padding='pre', maxlen=sent_length)\n",
    "# •\t→ Ensures all sentences are of the same length (8 words) otherwise it will not able to train the RNN model.\n",
    "# •\t→ padding='pre' adds zeros at the start if sentence is short.\n",
    "# •\t→ maxlen=8 sets the maximum length of the sentences to 8.\n",
    "# •\t→ Example:\n",
    "# o\t[0, 0, 0, 0, 6186, 6775, 637, 4895]\n",
    "\n",
    "print(embedded_docs)\n",
    "# [[   0    0    0    0 6086 1649 3854 2592]\n",
    "#  [   0    0    0    0 6086 1649 3854 7355]\n",
    "#  [   0    0    0    0 6086 3942 3854 9267]\n",
    "#  [   0    0    0 3828 1193  872 5319 2893]\n",
    "#  [   0    0    0 3828 1193  872 5319 6764]\n",
    "#  [   0    0    0  391 6086 7592 3854  464]\n",
    "#  [   0    0    0    0 1672 6762  789 5319]]\n",
    "\n",
    "\n",
    "# 🔹 Step 8: ⚙️ Setup Embedding Layer\n",
    "embedding_vector_features = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# •\t→ Embedding layer takes:\n",
    "# o\tvocab_size: total words it can handle\n",
    "# o\tembedding_vector_features: size of the dense vector (features) for each word\n",
    "# o\tinput_length: max sentence length (for RNNs or CNNs)\n",
    "\n",
    "# 🔹 Step 9: 📊 Model Summary\n",
    "model.summary()\n",
    "# Model: \"sequential_1\"\n",
    "# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "# ┃ Layer (type)                    ┃ Output Shape           ┃ Param #       ┃\n",
    "# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "# │ embedding_1 (Embedding)         │ ?                      │ 0 (unbuilt)   │\n",
    "# └─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "\n",
    "# 🔹 Step 10: 🧪 Predict Using Embedding\n",
    "print(model.predict(embedded_docs))\n",
    "# •\t→ Now each sentence is represented as a matrix of shape (8, 10):\n",
    "# o\t8 → max words per sentence\n",
    "# o\t10 → features per word\n",
    "# •\t→ Example: model.predict(embedded_docs[0].reshape(1, -1)) shows vector for sentence 'the glass of milk'\n",
    "\n",
    "# Output: \n",
    "# \u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
    "# [[[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [ 0.04040735  0.02275534 -0.02276136 -0.01484396  0.01204512\n",
    "#     0.03833229 -0.01339642 -0.00392485 -0.03046179  0.04022882]\n",
    "#   [-0.03019124 -0.01279249 -0.03905613 -0.02459447 -0.03395279\n",
    "#     0.00775326 -0.03828407 -0.04277759  0.02639576  0.04670269]\n",
    "#   [-0.00419534  0.04367883  0.01065139 -0.02015371 -0.04413842\n",
    "#    -0.01546397  0.01556754  0.01885882  0.00462196 -0.03924478]\n",
    "#   [-0.01765973 -0.02059596  0.01124931 -0.04790077 -0.03543241\n",
    "#     0.03486873  0.02037734  0.01445598 -0.04581702 -0.00621518]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [ 0.04040735  0.02275534 -0.02276136 -0.01484396  0.01204512\n",
    "#     0.03833229 -0.01339642 -0.00392485 -0.03046179  0.04022882]\n",
    "#   [-0.03019124 -0.01279249 -0.03905613 -0.02459447 -0.03395279\n",
    "#     0.00775326 -0.03828407 -0.04277759  0.02639576  0.04670269]\n",
    "#   [-0.00419534  0.04367883  0.01065139 -0.02015371 -0.04413842\n",
    "#    -0.01546397  0.01556754  0.01885882  0.00462196 -0.03924478]\n",
    "#   [-0.02429505 -0.018147   -0.03645756 -0.03517072 -0.00346982\n",
    "#    -0.00747031 -0.04896731  0.04250869 -0.03297199 -0.03667396]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [ 0.04040735  0.02275534 -0.02276136 -0.01484396  0.01204512\n",
    "#     0.03833229 -0.01339642 -0.00392485 -0.03046179  0.04022882]\n",
    "#   [-0.0497164   0.00153299  0.02029312  0.02193828 -0.01244128\n",
    "#     0.00724491 -0.00549623 -0.00725592 -0.00728454  0.01878544]\n",
    "#   [-0.00419534  0.04367883  0.01065139 -0.02015371 -0.04413842\n",
    "#    -0.01546397  0.01556754  0.01885882  0.00462196 -0.03924478]\n",
    "#   [-0.04448075 -0.01344184 -0.02768246 -0.03871102  0.02572754\n",
    "#    -0.00365255  0.02883886  0.01475826 -0.01304816  0.01194626]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.02810692  0.04585162  0.00783378  0.04826948  0.03797236\n",
    "#    -0.01758863 -0.00021721 -0.01458552 -0.04710324  0.04270247]\n",
    "#   [-0.02526909  0.00819306  0.01378684 -0.00490857  0.03530004\n",
    "#    -0.00541176  0.01337293  0.02394569 -0.04992829 -0.02327907]\n",
    "#   [ 0.03897044  0.00175347 -0.01567962 -0.03710892 -0.00812767\n",
    "#    -0.0126673  -0.00261028  0.04230764  0.04169044  0.02090956]\n",
    "#   [-0.0216205   0.01036555  0.02149669  0.04637566  0.04399173\n",
    "#    -0.00486566 -0.00116234 -0.01853008 -0.00501232  0.00057198]\n",
    "#   [-0.01953138  0.04612256 -0.0066989  -0.03013592 -0.00503711\n",
    "#     0.04142947  0.03435178 -0.04601353 -0.01902003 -0.02458649]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.02810692  0.04585162  0.00783378  0.04826948  0.03797236\n",
    "#    -0.01758863 -0.00021721 -0.01458552 -0.04710324  0.04270247]\n",
    "#   [-0.02526909  0.00819306  0.01378684 -0.00490857  0.03530004\n",
    "#    -0.00541176  0.01337293  0.02394569 -0.04992829 -0.02327907]\n",
    "#   [ 0.03897044  0.00175347 -0.01567962 -0.03710892 -0.00812767\n",
    "#    -0.0126673  -0.00261028  0.04230764  0.04169044  0.02090956]\n",
    "#   [-0.0216205   0.01036555  0.02149669  0.04637566  0.04399173\n",
    "#    -0.00486566 -0.00116234 -0.01853008 -0.00501232  0.00057198]\n",
    "#   [-0.01512501 -0.00613271  0.02041062  0.03420749 -0.02175893\n",
    "#    -0.01719315 -0.0423243  -0.011119   -0.01446674  0.04440813]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.0269259   0.01682637 -0.02316555 -0.02261239 -0.03732441\n",
    "#    -0.03082137 -0.03901649  0.01009659  0.03570462  0.04694809]\n",
    "#   [ 0.04040735  0.02275534 -0.02276136 -0.01484396  0.01204512\n",
    "#     0.03833229 -0.01339642 -0.00392485 -0.03046179  0.04022882]\n",
    "#   [-0.02979877  0.02704749 -0.04209245 -0.04393542  0.04377755\n",
    "#    -0.00807125  0.04097413 -0.01018187  0.03306102 -0.02885369]\n",
    "#   [-0.00419534  0.04367883  0.01065139 -0.02015371 -0.04413842\n",
    "#    -0.01546397  0.01556754  0.01885882  0.00462196 -0.03924478]\n",
    "#   [ 0.00940046  0.01597072 -0.04192784 -0.04229158  0.03902156\n",
    "#     0.02196207  0.04559286  0.0264777   0.03492266  0.00921279]]\n",
    "\n",
    "#  [[-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.03569493 -0.03568484 -0.01413916  0.03283079  0.02087626\n",
    "#    -0.02224137 -0.00520598 -0.03330121  0.0161608  -0.01954903]\n",
    "#   [-0.01064968 -0.0231376   0.03079159 -0.02931199 -0.03796649\n",
    "#     0.03909513  0.00260478  0.02335879  0.03717966 -0.03719858]\n",
    "#   [ 0.04222336 -0.02897574  0.04786757  0.04115654  0.01940531\n",
    "#    -0.02504121  0.02561492 -0.03015587  0.03786654 -0.03226081]\n",
    "#   [-0.01725054 -0.04744424  0.01454699 -0.03942645  0.04848105\n",
    "#    -0.04392425  0.01487465 -0.00022218  0.02037709 -0.04943595]\n",
    "#   [-0.0216205   0.01036555  0.02149669  0.04637566  0.04399173\n",
    "#    -0.00486566 -0.00116234 -0.01853008 -0.00501232  0.00057198]]]\n",
    "\n",
    "\n",
    "# 🔹 Step 11: 📍 What’s Happening?\n",
    "# •\t→ The sentence 'the glass of milk' is tokenized to [6186, 6775, 637, 4895]\n",
    "# •\t→ Padded to [0, 0, 0, 0, 6186, 6775, 637, 4895]\n",
    "# •\t→ Each of these numbers is now converted into a 10-dim vector by the embedding layer\n",
    "# •\t→ For example:\n",
    "# •\t6186 → [0.25, -0.14, 0.31, ..., 0.08]  → vector of size 10\n",
    "\n",
    "\n",
    "# 🔹 Step 12: 📌 Why Embedding is Better?\n",
    "# •\t→ Embedding gives you:\n",
    "# o\tDense vectors (more efficient than one-hot)\n",
    "# o\tPreserves semantic relationships\n",
    "# •\t→ Example:\n",
    "# o\tSimilar words like king and queen are close in vector space\n",
    "# o\tDifferent categories (like apple and mango) also cluster together\n",
    "\n",
    "embedded_docs[0]\n",
    "# array([   0,    0,    0,    0, 6086, 1649, 3854, 2592], dtype=int32)\n",
    "model.predict(embedded_docs[0].reshape(1, -1))\n",
    "# array([[[-0.02139236,  0.04156153, -0.00822502, -0.02136725,\n",
    "#           0.00672828,  0.04571738, -0.00378443, -0.0127629 ,\n",
    "#          -0.01274241,  0.0490505 ],\n",
    "#         [-0.02139236,  0.04156153, -0.00822502, -0.02136725,\n",
    "#           0.00672828,  0.04571738, -0.00378443, -0.0127629 ,\n",
    "#          -0.01274241,  0.0490505 ],\n",
    "#         [-0.02139236,  0.04156153, -0.00822502, -0.02136725,\n",
    "#           0.00672828,  0.04571738, -0.00378443, -0.0127629 ,\n",
    "#          -0.01274241,  0.0490505 ],\n",
    "#         [-0.02139236,  0.04156153, -0.00822502, -0.02136725,\n",
    "#           0.00672828,  0.04571738, -0.00378443, -0.0127629 ,\n",
    "#          -0.01274241,  0.0490505 ],\n",
    "#         [ 0.04731965,  0.01597354,  0.04082378,  0.0327131 ,\n",
    "#           0.00288612,  0.04806567,  0.04102972,  0.03775961,\n",
    "#          -0.02092441,  0.02246917],\n",
    "#         [-0.00382866,  0.02513489, -0.01869538,  0.0055298 ,\n",
    "#           0.02396252, -0.0181849 ,  0.04099594,  0.03989681,\n",
    "#           0.01045933,  0.00703907],\n",
    "#         [-0.00116382,  0.04169172,  0.00152386, -0.03997656,\n",
    "#          -0.02834749, -0.00111879,  0.00158717, -0.00380088,\n",
    "#          -0.03648036, -0.02725489],\n",
    "#         [ 0.04764913,  0.04994391,  0.0279489 , -0.01208482,\n",
    "#          -0.03168216, -0.03444691,  0.0311343 ,  0.02132369,\n",
    "#           0.01532625, -0.0248673 ]]], dtype=float32)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
