{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1516f0",
   "metadata": {},
   "source": [
    "'''END to ENd Deep Learning using Simple RNN'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (25000,), Training labels shape: (25000,)\n",
      "Testing data shape: (25000,), Testing labels shape: (25000,)\n",
      "25000 25000\n",
      "Sample review as integers: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Sample label: 1\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "# üîπ Libraries to Import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "# ‚Ä¢\tnumpy: For numerical operations.\n",
    "# ‚Ä¢\ttensorflow & keras: For building the neural network.\n",
    "# ‚Ä¢\timdb: Built-in dataset of TensorFlow.\n",
    "# ‚Ä¢\tsequence: For padding sequences (to make sentence lengths equal).\n",
    "# ‚Ä¢\tSequential: Used to create the model step-by-step.\n",
    "# ‚Ä¢\tEmbedding: Turns word indexes into dense vectors.\n",
    "# ‚Ä¢\tSimpleRNN: Recurrent Neural Network layer.\n",
    "# ‚Ä¢\tDense: Fully connected layer for classification.\n",
    "\n",
    "\n",
    "# üîπ Load and Prepare the Dataset\n",
    "max_features = 10000  # Vocabulary size (top 10,000 frequent words)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# ‚Ä¢\tWe are using only top 10,000 most frequent words.\n",
    "# ‚Ä¢\tx_train: List of movie reviews as integer word indexes.\n",
    "# ‚Ä¢\ty_train: Labels ‚Üí 1 (positive), 0 (negative).\n",
    "# ‚úÖ Print dataset size:\n",
    "# Print the shape of the data\n",
    "print(f'Training data shape: {x_train.shape}, Training labels shape: {y_train.shape}')\n",
    "print(f'Testing data shape: {x_test.shape}, Testing labels shape: {y_test.shape}')\n",
    "print(len(x_train), len(x_test))  # 25000 training and 25000 testing reviews\n",
    "# Training data shape: (25000,), Training labels shape: (25000,)\n",
    "# Testing data shape: (25000,), Testing labels shape: (25000,)\n",
    "# 25000 25000\n",
    "\n",
    "# üîπ Look at One Example Review\n",
    "sample_review = x_train[0]\n",
    "sample_label = y_train[0]\n",
    "# ‚Ä¢\tx_train[0] ‚Üí A review as a list of integers.\n",
    "# üìå These are not actual words but indexes of the words.\n",
    "# üîç Example:\n",
    "print(\"Sample review as integers:\", sample_review)\n",
    "print(\"Sample label:\", sample_label)  # 1 = Positive review\n",
    "# Sample review as integers: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
    "# Sample label: 1\n",
    "\n",
    "\n",
    "# üîπ Convert Integer Indexes Back to Words\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "# ‚Ä¢\tword_index: A dictionary mapping words to their indexes.\n",
    "# ‚Ä¢\treverse_word_index: Reverse mapping (index ‚Üí word).\n",
    "# ‚úÖ Decode the review:\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in sample_review])\n",
    "print(decoded_review)\n",
    "# ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
    "\n",
    "# ‚Ä¢\tWhy i - 3?\n",
    "# o\tBecause the first 3 indexes (0,1,2) are reserved for padding, start, and unknown tokens.\n",
    "# ‚Ä¢\tIf a word index is not found ‚Üí ? is shown.\n",
    "# üîç Example output:\n",
    "# ? this film was just brilliant casting location story direction ...\n",
    "\n",
    "# üîπ Pad Sequences (Make All Reviews Same Length)\n",
    "\n",
    "max_length = 500  # Limit every review to 500 words\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "# ‚Ä¢\tShorter reviews are padded with 0s at the beginning (pre-padding).\n",
    "# ‚Ä¢\tThis ensures all reviews are of equal length = 500 words.\n",
    "# üîç Check an example after padding:\n",
    "print(x_train[0])\n",
    "# üìå Output:\n",
    "# [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "#     0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
    "#    66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
    "#     2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
    "#    39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
    "#     6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
    "#    12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
    "#    12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
    "#     4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
    "#     5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
    "#    28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
    "#    15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
    "#   400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
    "#    98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
    "#   134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
    "#    25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
    "#  4472  113  103   32   15   16 5345   19  178   32]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c342e",
   "metadata": {},
   "source": [
    "'''Train Simple RNN Model on IMDB Reviews'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0206d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 166ms/step - accuracy: 0.6281 - loss: 51328021692416.0000 - val_accuracy: 0.6294 - val_loss: 0.6551\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 183ms/step - accuracy: 0.5788 - loss: 0.7105 - val_accuracy: 0.5338 - val_loss: 0.7310\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 173ms/step - accuracy: 0.6382 - loss: 0.6346 - val_accuracy: 0.5842 - val_loss: 0.6551\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 181ms/step - accuracy: 0.6794 - loss: 0.6031 - val_accuracy: 0.5776 - val_loss: 0.6492\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 180ms/step - accuracy: 0.7049 - loss: 0.5834 - val_accuracy: 0.5940 - val_loss: 0.6505\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 240ms/step - accuracy: 0.7225 - loss: 0.5634 - val_accuracy: 0.6230 - val_loss: 0.6335\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 250ms/step - accuracy: 0.7366 - loss: 0.5451 - val_accuracy: 0.6062 - val_loss: 0.6434\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 270ms/step - accuracy: 0.7387 - loss: 0.5328 - val_accuracy: 0.6258 - val_loss: 0.6366\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 284ms/step - accuracy: 0.7594 - loss: 0.5172 - val_accuracy: 0.6302 - val_loss: 0.6345\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 170ms/step - accuracy: 0.7635 - loss: 0.5026 - val_accuracy: 0.6294 - val_loss: 0.6366\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    " # üìè Sequential Model is used to create a sequential model.\n",
    "# Why we need a Sequential model?\n",
    "# ‚Ä¢\t‚Üí Sequential model is used to create a sequential model.\n",
    "\n",
    "# üîπ Add Embedding Layer\n",
    "model.add(Embedding(max_features, 128, input_length=max_length))\n",
    "# ‚Ä¢\tüìò Embedding layer helps in converting each word (represented by an index) into a dense vector of fixed size.\n",
    "# ‚Ä¢\tmax_features: Vocabulary size (top 10,000 words).\n",
    "# ‚Ä¢\t128: Number of dimensions for the vector representation of each word.\n",
    "# ‚Ä¢\tinput_length: Max number of words in a sentence (here, 500).\n",
    "# üìù So, each sentence will be converted into a 500 √ó 128 matrix.\n",
    "\n",
    "# üîπ Add Simple RNN Layer\n",
    "model.add(SimpleRNN(128, activation='relu'))\n",
    "# ‚Ä¢\tüîÅ SimpleRNN processes sequence data and keeps track of what it has seen before.\n",
    "# ‚Ä¢\t128: Number of RNN units (neurons).\n",
    "# ‚Ä¢\tactivation='relu': Activation function for the layer.\n",
    "# üìù RNN helps in understanding the sequence and relationships between words.\n",
    "\n",
    "# üîπ Add Output Layer (Dense)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# ‚Ä¢\tüéØ Dense layer gives the final output.\n",
    "# ‚Ä¢\t1: One output node (since it's binary classification: Positive or Negative).\n",
    "# ‚Ä¢\tsigmoid: Converts the output to a probability between 0 and 1.\n",
    "\n",
    "# üîπ Model Summary\n",
    "model.summary()\n",
    "# Model: \"sequential\"\n",
    "# ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
    "# ‚îÉ Layer (type)                   ‚îÉ Output Shape           ‚îÉ     Param #   ‚îÉ\n",
    "# ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
    "# ‚îÇ embedding (Embedding)          ‚îÇ ?                      ‚îÇ     0 (unbuilt)‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "# ‚îÇ simple_rnn (SimpleRNN)         ‚îÇ ?                      ‚îÇ     0 (unbuilt)‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "# ‚îÇ dense (Dense)                  ‚îÇ ?                      ‚îÇ     0 (unbuilt)‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\n",
    "# ‚Ä¢\tüìä Shows a table with layer types, shapes, and number of parameters.\n",
    "# ‚Ä¢\tHelps you understand how many total parameters your model is training.\n",
    "# üìù If it were a multi-class problem, we would use Dense(num_classes, activation='softmax').\n",
    "\n",
    "# üîπ Compile the Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# ‚Ä¢\t‚öôÔ∏è This step prepares the model for training.\n",
    "# ‚Ä¢\toptimizer='adam': Adam optimizer is efficient and adaptive.\n",
    "# ‚Ä¢\tloss='binary_crossentropy': Used for binary classification.\n",
    "# ‚Ä¢\tmetrics=['accuracy']: Monitor accuracy during training.\n",
    "\n",
    "\n",
    "# üîπ Set Up Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # üìè Monitor the validation type of loss which minimizes during training.\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "# ‚Ä¢\tüõë EarlyStopping stops training when the model stops improving on validation data.\n",
    "# ‚Ä¢\tmonitor='val_loss': Watch the validation loss.\n",
    "# ‚Ä¢\tpatience=5: Wait 5 epochs before stopping if no improvement.\n",
    "# ‚Ä¢\trestore_best_weights=True: After stopping, load the best weights (where val_loss was minimum).\n",
    "# üìù This prevents overfitting and saves training time.\n",
    "\n",
    "\n",
    "# üîπ Train the Model with Early Stopping\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# ‚Ä¢\tüéì This is where the model learns from data.\n",
    "# ‚Ä¢\tx_train, y_train: Training data and labels.\n",
    "# ‚Ä¢\tepochs=10: Train for 10 cycles.\n",
    "# ‚Ä¢\tbatch_size=32: Process 32 samples at a time.\n",
    "# ‚Ä¢\tvalidation_split=0.2: 20% of training data used for validation.\n",
    "# ‚Ä¢\tcallbacks=[early_stopping]: Monitor training and apply early stopping.\n",
    "# üìù This training will take some time, especially with 25,000 records.\n",
    "# With every epoch, the model becomes better at predicting the correct output (hopefully!).\n",
    "# The model won‚Äôt learn from this 20% ‚Äî it only uses it to evaluate how well it's doing while training.\n",
    "\n",
    "\n",
    "# Epoch 1/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 105s 166ms/step - accuracy: 0.6281 - loss: 51328021692416.0000 - val_accuracy: 0.6294 - val_loss: 0.6551\n",
    "# Epoch 2/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 114s 183ms/step - accuracy: 0.5788 - loss: 0.7105 - val_accuracy: 0.5338 - val_loss: 0.7310\n",
    "# Epoch 3/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 108s 173ms/step - accuracy: 0.6382 - loss: 0.6346 - val_accuracy: 0.5842 - val_loss: 0.6551\n",
    "# Epoch 4/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 113s 181ms/step - accuracy: 0.6794 - loss: 0.6031 - val_accuracy: 0.5776 - val_loss: 0.6492\n",
    "# Epoch 5/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 113s 180ms/step - accuracy: 0.7049 - loss: 0.5834 - val_accuracy: 0.5940 - val_loss: 0.6505\n",
    "# Epoch 6/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 150s 240ms/step - accuracy: 0.7225 - loss: 0.5634 - val_accuracy: 0.6230 - val_loss: 0.6335\n",
    "# Epoch 7/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 156s 250ms/step - accuracy: 0.7366 - loss: 0.5451 - val_accuracy: 0.6062 - val_loss: 0.6434\n",
    "# Epoch 8/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 169s 270ms/step - accuracy: 0.7387 - loss: 0.5328 - val_accuracy: 0.6258 - val_loss: 0.6366\n",
    "# Epoch 9/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 178s 284ms/step - accuracy: 0.7594 - loss: 0.5172 - val_accuracy: 0.6302 - val_loss: 0.6345\n",
    "# Epoch 10/10\n",
    "# 625/625 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 107s 170ms/step - accuracy: 0.7635 - loss: 0.5026 - val_accuracy: 0.6294 - val_loss: 0.6366\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58631810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# ‚úîÔ∏è Saving the Model\n",
    "# ‚Ä¢\tüíæ Now the model is saved using the .h5 format.\n",
    "# ‚Ä¢\tüìÅ We can save logs too and visualize them using TensorBoard.\n",
    "# ‚Ä¢\tüí° Command used:\n",
    "model.save(\"simple_RNN_IMDb.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
